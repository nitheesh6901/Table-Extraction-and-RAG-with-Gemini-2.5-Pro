{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7882f9e7-416f-4e4f-98d2-a23874fc6e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90e0303f-bbf0-4e2c-830d-12760eb0531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder = Path(\"pdfs\")                           # The datasets stored\n",
    "output_csv_folder = Path(\"tables_csv\")              # Extracted tables data in csv\n",
    "output_csv_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff93bab6-45f9-49fa-8fcc-1f03d277cafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables_text = []\n",
    "sources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8735f3d6-37ea-4542-87c5-2d4650d1d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1 PDFs to process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_files = list(pdf_folder.glob(\"*.pdf\"))\n",
    "print(f\"found {len(pdf_files)} PDFs to process.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138a9e18-ecb1-4548-9bdd-677082e8cdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing PDF : foo.pdf\n",
      " extracted 1 tables\n",
      "\n",
      " Extracted total 1 tables.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pdf_file in pdf_files:                                          # Extracting tables using camelot\n",
    "    print(f\"processing PDF : {pdf_file.name}\")\n",
    "    try:\n",
    "        tables = camelot.read_pdf(str(pdf_file),pages=\"all\",flavor=\"lattice\")\n",
    "        for i, table in enumerate(tables):\n",
    "            csv_path = output_csv_folder/f\"{pdf_file.stem}_table{i+1}.csv\"\n",
    "            table.to_csv(str(csv_path))\n",
    "\n",
    "            df = table.df                                          # Convert table data into a DataFrame\n",
    "            table_text = df.to_string(index=False)                 # Convert DataFrame to plain text for embedding\n",
    "            all_tables_text.append(table_text)\n",
    "            sources.append(f\"{pdf_file.name} - Table{i+1} \")\n",
    "        print(f\" extracted {len(tables)} tables\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to process {pdf_file.name}: {e}\")\n",
    "        \n",
    "print(f\"\\n Extracted total {len(all_tables_text)} tables.\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96a27a7c-e422-4524-b7b2-9990fb02e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nithe\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8083d800-a1ac-4983-8ddf-8c34b8e06411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28882f6e6c0844a796f420abb00b88f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")                         # Encode all table texts into numerical vector embeddings\n",
    "embeddings = model.encode(all_tables_text,convert_to_numpy=True,show_progress_bar=True)\n",
    "embedding_dim = embeddings.shape[1]\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d2ffe5-c97e-49c3-9095-af7c42f0b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91105b99-2b59-46ec-9dbe-389d5580f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FAISS index built with 1 vectors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = faiss.IndexFlatL2(embedding_dim)                      # Initialize a FAISS index (L2 distance) for fast vector similarity search\n",
    "index.add(embeddings)                                         # Add all embeddings to the FAISS index\n",
    "print(f\" FAISS index built with {index.ntotal} vectors.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9e30f95-bd31-4af7-924c-79e3f3e30f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2a1da62-ebf9-4daf-a5d8-9cc7d2817421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer:\n",
      " Cycle 4171_1 has the highest Percent Fuel Savings, with a 58.1% savings from Improved Speed.\n",
      "\n",
      " Sources used: ['foo.pdf - Table1 ', 'foo.pdf - Table1 ', 'foo.pdf - Table1 ']\n"
     ]
    }
   ],
   "source": [
    "genai.configure(api_key = \"AIzaSyC6uE-JcK8Ybu3sQA4jpYX12LEJWysB2HU\")     # Configure the Gemini API with my API key\n",
    "\n",
    "def query_table(query, k=3):\n",
    "    query_emb = model.encode([query],convert_to_numpy=True)               # Convert the query into an embedding vector\n",
    "    distances,indices = index.search(query_emb,k)                         # Retrieve the top-k most similar tables from FAISS\n",
    "    context = \"\\n\\n\".join([all_tables_text[i] for i in indices[0]])\n",
    "    used_sources = [sources[i] for i in indices[0]]\n",
    "\n",
    "     # Build the prompt for the LLM (Gemini)\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant helping to answer questions from extracted tables.                   \n",
    "Use the following tables as your context and provide a clear, concise answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "    model_gemini = genai.GenerativeModel(\"gemini-2.5-pro\")                \n",
    "    response = model_gemini.generate_content(prompt)\n",
    "    \n",
    "    return response.text.strip(), used_sources\n",
    "     \n",
    "query = \"Which cycle has the highest Percent Fuel Savings?\"                   # Example query\n",
    "answer, used_sources = query_table(query)\n",
    "\n",
    "print(\"\\n Answer:\\n\", answer)\n",
    "print(\"\\n Sources used:\", used_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611a3bb-ef55-44dc-8eb2-ff2d898c4c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
